{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVision FINAL PROJECT JT and Alex.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlff81UQAjKM",
        "colab_type": "text"
      },
      "source": [
        "**JT Graass and Alex Wassel's Computer Vision Final Project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttn1L-1lEdKB",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preparation and Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpMDmcL3nXJm",
        "colab_type": "text"
      },
      "source": [
        "We used the MNIST Sign Language dataset from Kaggle. These images of hands in various letter formations have only one color channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cixnUI2KjfYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# from datasets import sign_language\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "# import kaggle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy08eVOkjueB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('./sign_mnist_train.csv')\n",
        "test = pd.read_csv('./sign_mnist_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPgnEmtHnMhG",
        "colab_type": "code",
        "outputId": "8bd88871-189d-4ed0-e7ed-9fd6fed40d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>149</td>\n",
              "      <td>149</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>151</td>\n",
              "      <td>151</td>\n",
              "      <td>150</td>\n",
              "      <td>151</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>153</td>\n",
              "      <td>153</td>\n",
              "      <td>151</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>153</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>151</td>\n",
              "      <td>151</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>149</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>151</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>152</td>\n",
              "      <td>...</td>\n",
              "      <td>131</td>\n",
              "      <td>134</td>\n",
              "      <td>144</td>\n",
              "      <td>147</td>\n",
              "      <td>125</td>\n",
              "      <td>87</td>\n",
              "      <td>87</td>\n",
              "      <td>103</td>\n",
              "      <td>107</td>\n",
              "      <td>110</td>\n",
              "      <td>116</td>\n",
              "      <td>113</td>\n",
              "      <td>75</td>\n",
              "      <td>74</td>\n",
              "      <td>74</td>\n",
              "      <td>74</td>\n",
              "      <td>76</td>\n",
              "      <td>74</td>\n",
              "      <td>82</td>\n",
              "      <td>134</td>\n",
              "      <td>168</td>\n",
              "      <td>155</td>\n",
              "      <td>146</td>\n",
              "      <td>137</td>\n",
              "      <td>145</td>\n",
              "      <td>146</td>\n",
              "      <td>149</td>\n",
              "      <td>135</td>\n",
              "      <td>124</td>\n",
              "      <td>125</td>\n",
              "      <td>138</td>\n",
              "      <td>148</td>\n",
              "      <td>127</td>\n",
              "      <td>89</td>\n",
              "      <td>82</td>\n",
              "      <td>96</td>\n",
              "      <td>106</td>\n",
              "      <td>112</td>\n",
              "      <td>120</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>126</td>\n",
              "      <td>128</td>\n",
              "      <td>131</td>\n",
              "      <td>132</td>\n",
              "      <td>133</td>\n",
              "      <td>134</td>\n",
              "      <td>135</td>\n",
              "      <td>135</td>\n",
              "      <td>136</td>\n",
              "      <td>138</td>\n",
              "      <td>137</td>\n",
              "      <td>137</td>\n",
              "      <td>138</td>\n",
              "      <td>138</td>\n",
              "      <td>139</td>\n",
              "      <td>137</td>\n",
              "      <td>142</td>\n",
              "      <td>140</td>\n",
              "      <td>138</td>\n",
              "      <td>139</td>\n",
              "      <td>137</td>\n",
              "      <td>137</td>\n",
              "      <td>136</td>\n",
              "      <td>135</td>\n",
              "      <td>134</td>\n",
              "      <td>133</td>\n",
              "      <td>134</td>\n",
              "      <td>132</td>\n",
              "      <td>129</td>\n",
              "      <td>132</td>\n",
              "      <td>134</td>\n",
              "      <td>135</td>\n",
              "      <td>135</td>\n",
              "      <td>137</td>\n",
              "      <td>139</td>\n",
              "      <td>139</td>\n",
              "      <td>139</td>\n",
              "      <td>140</td>\n",
              "      <td>141</td>\n",
              "      <td>...</td>\n",
              "      <td>114</td>\n",
              "      <td>112</td>\n",
              "      <td>89</td>\n",
              "      <td>48</td>\n",
              "      <td>133</td>\n",
              "      <td>194</td>\n",
              "      <td>182</td>\n",
              "      <td>185</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>182</td>\n",
              "      <td>181</td>\n",
              "      <td>172</td>\n",
              "      <td>174</td>\n",
              "      <td>177</td>\n",
              "      <td>178</td>\n",
              "      <td>178</td>\n",
              "      <td>179</td>\n",
              "      <td>181</td>\n",
              "      <td>183</td>\n",
              "      <td>187</td>\n",
              "      <td>175</td>\n",
              "      <td>165</td>\n",
              "      <td>154</td>\n",
              "      <td>118</td>\n",
              "      <td>107</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>96</td>\n",
              "      <td>83</td>\n",
              "      <td>47</td>\n",
              "      <td>104</td>\n",
              "      <td>194</td>\n",
              "      <td>183</td>\n",
              "      <td>186</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>182</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>85</td>\n",
              "      <td>88</td>\n",
              "      <td>92</td>\n",
              "      <td>96</td>\n",
              "      <td>105</td>\n",
              "      <td>123</td>\n",
              "      <td>135</td>\n",
              "      <td>143</td>\n",
              "      <td>147</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>171</td>\n",
              "      <td>182</td>\n",
              "      <td>172</td>\n",
              "      <td>175</td>\n",
              "      <td>185</td>\n",
              "      <td>183</td>\n",
              "      <td>184</td>\n",
              "      <td>185</td>\n",
              "      <td>185</td>\n",
              "      <td>185</td>\n",
              "      <td>183</td>\n",
              "      <td>183</td>\n",
              "      <td>182</td>\n",
              "      <td>181</td>\n",
              "      <td>178</td>\n",
              "      <td>86</td>\n",
              "      <td>88</td>\n",
              "      <td>93</td>\n",
              "      <td>96</td>\n",
              "      <td>108</td>\n",
              "      <td>125</td>\n",
              "      <td>137</td>\n",
              "      <td>145</td>\n",
              "      <td>149</td>\n",
              "      <td>154</td>\n",
              "      <td>160</td>\n",
              "      <td>...</td>\n",
              "      <td>145</td>\n",
              "      <td>123</td>\n",
              "      <td>78</td>\n",
              "      <td>162</td>\n",
              "      <td>239</td>\n",
              "      <td>227</td>\n",
              "      <td>229</td>\n",
              "      <td>226</td>\n",
              "      <td>226</td>\n",
              "      <td>225</td>\n",
              "      <td>224</td>\n",
              "      <td>222</td>\n",
              "      <td>89</td>\n",
              "      <td>91</td>\n",
              "      <td>94</td>\n",
              "      <td>111</td>\n",
              "      <td>136</td>\n",
              "      <td>154</td>\n",
              "      <td>167</td>\n",
              "      <td>184</td>\n",
              "      <td>125</td>\n",
              "      <td>3</td>\n",
              "      <td>166</td>\n",
              "      <td>225</td>\n",
              "      <td>195</td>\n",
              "      <td>188</td>\n",
              "      <td>172</td>\n",
              "      <td>185</td>\n",
              "      <td>161</td>\n",
              "      <td>122</td>\n",
              "      <td>68</td>\n",
              "      <td>166</td>\n",
              "      <td>242</td>\n",
              "      <td>227</td>\n",
              "      <td>230</td>\n",
              "      <td>227</td>\n",
              "      <td>226</td>\n",
              "      <td>225</td>\n",
              "      <td>224</td>\n",
              "      <td>222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>203</td>\n",
              "      <td>205</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>207</td>\n",
              "      <td>209</td>\n",
              "      <td>210</td>\n",
              "      <td>209</td>\n",
              "      <td>210</td>\n",
              "      <td>209</td>\n",
              "      <td>208</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>209</td>\n",
              "      <td>208</td>\n",
              "      <td>210</td>\n",
              "      <td>210</td>\n",
              "      <td>207</td>\n",
              "      <td>209</td>\n",
              "      <td>209</td>\n",
              "      <td>208</td>\n",
              "      <td>209</td>\n",
              "      <td>210</td>\n",
              "      <td>209</td>\n",
              "      <td>207</td>\n",
              "      <td>208</td>\n",
              "      <td>209</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>208</td>\n",
              "      <td>209</td>\n",
              "      <td>208</td>\n",
              "      <td>208</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>209</td>\n",
              "      <td>209</td>\n",
              "      <td>...</td>\n",
              "      <td>85</td>\n",
              "      <td>80</td>\n",
              "      <td>84</td>\n",
              "      <td>151</td>\n",
              "      <td>238</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>250</td>\n",
              "      <td>237</td>\n",
              "      <td>245</td>\n",
              "      <td>250</td>\n",
              "      <td>232</td>\n",
              "      <td>103</td>\n",
              "      <td>101</td>\n",
              "      <td>102</td>\n",
              "      <td>103</td>\n",
              "      <td>95</td>\n",
              "      <td>208</td>\n",
              "      <td>231</td>\n",
              "      <td>227</td>\n",
              "      <td>209</td>\n",
              "      <td>190</td>\n",
              "      <td>179</td>\n",
              "      <td>182</td>\n",
              "      <td>152</td>\n",
              "      <td>150</td>\n",
              "      <td>159</td>\n",
              "      <td>119</td>\n",
              "      <td>83</td>\n",
              "      <td>63</td>\n",
              "      <td>154</td>\n",
              "      <td>248</td>\n",
              "      <td>247</td>\n",
              "      <td>248</td>\n",
              "      <td>253</td>\n",
              "      <td>236</td>\n",
              "      <td>230</td>\n",
              "      <td>240</td>\n",
              "      <td>253</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>188</td>\n",
              "      <td>191</td>\n",
              "      <td>193</td>\n",
              "      <td>195</td>\n",
              "      <td>199</td>\n",
              "      <td>201</td>\n",
              "      <td>202</td>\n",
              "      <td>203</td>\n",
              "      <td>203</td>\n",
              "      <td>203</td>\n",
              "      <td>204</td>\n",
              "      <td>204</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>198</td>\n",
              "      <td>216</td>\n",
              "      <td>217</td>\n",
              "      <td>135</td>\n",
              "      <td>181</td>\n",
              "      <td>200</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>193</td>\n",
              "      <td>190</td>\n",
              "      <td>189</td>\n",
              "      <td>187</td>\n",
              "      <td>185</td>\n",
              "      <td>190</td>\n",
              "      <td>194</td>\n",
              "      <td>196</td>\n",
              "      <td>197</td>\n",
              "      <td>200</td>\n",
              "      <td>202</td>\n",
              "      <td>204</td>\n",
              "      <td>206</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>...</td>\n",
              "      <td>93</td>\n",
              "      <td>52</td>\n",
              "      <td>24</td>\n",
              "      <td>53</td>\n",
              "      <td>63</td>\n",
              "      <td>33</td>\n",
              "      <td>41</td>\n",
              "      <td>51</td>\n",
              "      <td>48</td>\n",
              "      <td>45</td>\n",
              "      <td>49</td>\n",
              "      <td>55</td>\n",
              "      <td>149</td>\n",
              "      <td>150</td>\n",
              "      <td>150</td>\n",
              "      <td>148</td>\n",
              "      <td>147</td>\n",
              "      <td>151</td>\n",
              "      <td>124</td>\n",
              "      <td>82</td>\n",
              "      <td>84</td>\n",
              "      <td>81</td>\n",
              "      <td>69</td>\n",
              "      <td>81</td>\n",
              "      <td>111</td>\n",
              "      <td>103</td>\n",
              "      <td>84</td>\n",
              "      <td>75</td>\n",
              "      <td>53</td>\n",
              "      <td>28</td>\n",
              "      <td>26</td>\n",
              "      <td>40</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>29</td>\n",
              "      <td>46</td>\n",
              "      <td>49</td>\n",
              "      <td>46</td>\n",
              "      <td>46</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  ...  pixel781  pixel782  pixel783  pixel784\n",
              "0      6     149     149     150  ...       106       112       120       107\n",
              "1      5     126     128     131  ...       184       184       182       180\n",
              "2     10      85      88      92  ...       226       225       224       222\n",
              "3      0     203     205     207  ...       230       240       253       255\n",
              "4      3     188     191     193  ...        49        46        46        53\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-zqiMGunN78",
        "colab_type": "code",
        "outputId": "6135ddbf-e9bf-4733-fcc1-d7bbd91c7559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>107</td>\n",
              "      <td>118</td>\n",
              "      <td>127</td>\n",
              "      <td>134</td>\n",
              "      <td>139</td>\n",
              "      <td>143</td>\n",
              "      <td>146</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "      <td>165</td>\n",
              "      <td>159</td>\n",
              "      <td>166</td>\n",
              "      <td>168</td>\n",
              "      <td>170</td>\n",
              "      <td>170</td>\n",
              "      <td>171</td>\n",
              "      <td>171</td>\n",
              "      <td>171</td>\n",
              "      <td>172</td>\n",
              "      <td>171</td>\n",
              "      <td>171</td>\n",
              "      <td>170</td>\n",
              "      <td>170</td>\n",
              "      <td>169</td>\n",
              "      <td>111</td>\n",
              "      <td>121</td>\n",
              "      <td>129</td>\n",
              "      <td>135</td>\n",
              "      <td>141</td>\n",
              "      <td>144</td>\n",
              "      <td>148</td>\n",
              "      <td>151</td>\n",
              "      <td>154</td>\n",
              "      <td>157</td>\n",
              "      <td>160</td>\n",
              "      <td>...</td>\n",
              "      <td>205</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>204</td>\n",
              "      <td>205</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>142</td>\n",
              "      <td>151</td>\n",
              "      <td>160</td>\n",
              "      <td>172</td>\n",
              "      <td>196</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>190</td>\n",
              "      <td>135</td>\n",
              "      <td>96</td>\n",
              "      <td>86</td>\n",
              "      <td>77</td>\n",
              "      <td>77</td>\n",
              "      <td>79</td>\n",
              "      <td>176</td>\n",
              "      <td>205</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>155</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>158</td>\n",
              "      <td>157</td>\n",
              "      <td>158</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>154</td>\n",
              "      <td>153</td>\n",
              "      <td>152</td>\n",
              "      <td>151</td>\n",
              "      <td>149</td>\n",
              "      <td>149</td>\n",
              "      <td>148</td>\n",
              "      <td>147</td>\n",
              "      <td>146</td>\n",
              "      <td>144</td>\n",
              "      <td>142</td>\n",
              "      <td>143</td>\n",
              "      <td>138</td>\n",
              "      <td>92</td>\n",
              "      <td>108</td>\n",
              "      <td>158</td>\n",
              "      <td>159</td>\n",
              "      <td>159</td>\n",
              "      <td>159</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>...</td>\n",
              "      <td>100</td>\n",
              "      <td>78</td>\n",
              "      <td>120</td>\n",
              "      <td>157</td>\n",
              "      <td>168</td>\n",
              "      <td>107</td>\n",
              "      <td>99</td>\n",
              "      <td>121</td>\n",
              "      <td>133</td>\n",
              "      <td>97</td>\n",
              "      <td>95</td>\n",
              "      <td>120</td>\n",
              "      <td>135</td>\n",
              "      <td>116</td>\n",
              "      <td>95</td>\n",
              "      <td>79</td>\n",
              "      <td>69</td>\n",
              "      <td>86</td>\n",
              "      <td>139</td>\n",
              "      <td>173</td>\n",
              "      <td>200</td>\n",
              "      <td>185</td>\n",
              "      <td>175</td>\n",
              "      <td>198</td>\n",
              "      <td>124</td>\n",
              "      <td>118</td>\n",
              "      <td>94</td>\n",
              "      <td>140</td>\n",
              "      <td>133</td>\n",
              "      <td>84</td>\n",
              "      <td>69</td>\n",
              "      <td>149</td>\n",
              "      <td>128</td>\n",
              "      <td>87</td>\n",
              "      <td>94</td>\n",
              "      <td>163</td>\n",
              "      <td>175</td>\n",
              "      <td>103</td>\n",
              "      <td>135</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>185</td>\n",
              "      <td>185</td>\n",
              "      <td>185</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>181</td>\n",
              "      <td>181</td>\n",
              "      <td>179</td>\n",
              "      <td>179</td>\n",
              "      <td>179</td>\n",
              "      <td>178</td>\n",
              "      <td>178</td>\n",
              "      <td>109</td>\n",
              "      <td>52</td>\n",
              "      <td>66</td>\n",
              "      <td>77</td>\n",
              "      <td>83</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>...</td>\n",
              "      <td>203</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>196</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>193</td>\n",
              "      <td>198</td>\n",
              "      <td>166</td>\n",
              "      <td>132</td>\n",
              "      <td>114</td>\n",
              "      <td>89</td>\n",
              "      <td>74</td>\n",
              "      <td>79</td>\n",
              "      <td>77</td>\n",
              "      <td>74</td>\n",
              "      <td>78</td>\n",
              "      <td>132</td>\n",
              "      <td>188</td>\n",
              "      <td>210</td>\n",
              "      <td>209</td>\n",
              "      <td>206</td>\n",
              "      <td>205</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>209</td>\n",
              "      <td>207</td>\n",
              "      <td>208</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>198</td>\n",
              "      <td>197</td>\n",
              "      <td>195</td>\n",
              "      <td>192</td>\n",
              "      <td>197</td>\n",
              "      <td>171</td>\n",
              "      <td>51</td>\n",
              "      <td>52</td>\n",
              "      <td>54</td>\n",
              "      <td>212</td>\n",
              "      <td>213</td>\n",
              "      <td>215</td>\n",
              "      <td>215</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>213</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>...</td>\n",
              "      <td>247</td>\n",
              "      <td>242</td>\n",
              "      <td>233</td>\n",
              "      <td>231</td>\n",
              "      <td>230</td>\n",
              "      <td>229</td>\n",
              "      <td>227</td>\n",
              "      <td>225</td>\n",
              "      <td>223</td>\n",
              "      <td>221</td>\n",
              "      <td>220</td>\n",
              "      <td>216</td>\n",
              "      <td>58</td>\n",
              "      <td>51</td>\n",
              "      <td>49</td>\n",
              "      <td>50</td>\n",
              "      <td>57</td>\n",
              "      <td>60</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>159</td>\n",
              "      <td>255</td>\n",
              "      <td>237</td>\n",
              "      <td>239</td>\n",
              "      <td>237</td>\n",
              "      <td>236</td>\n",
              "      <td>235</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>231</td>\n",
              "      <td>230</td>\n",
              "      <td>226</td>\n",
              "      <td>225</td>\n",
              "      <td>222</td>\n",
              "      <td>229</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>164</td>\n",
              "      <td>167</td>\n",
              "      <td>170</td>\n",
              "      <td>172</td>\n",
              "      <td>176</td>\n",
              "      <td>179</td>\n",
              "      <td>180</td>\n",
              "      <td>184</td>\n",
              "      <td>185</td>\n",
              "      <td>186</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>190</td>\n",
              "      <td>191</td>\n",
              "      <td>189</td>\n",
              "      <td>190</td>\n",
              "      <td>190</td>\n",
              "      <td>187</td>\n",
              "      <td>190</td>\n",
              "      <td>192</td>\n",
              "      <td>193</td>\n",
              "      <td>191</td>\n",
              "      <td>191</td>\n",
              "      <td>192</td>\n",
              "      <td>192</td>\n",
              "      <td>194</td>\n",
              "      <td>194</td>\n",
              "      <td>166</td>\n",
              "      <td>169</td>\n",
              "      <td>172</td>\n",
              "      <td>174</td>\n",
              "      <td>177</td>\n",
              "      <td>180</td>\n",
              "      <td>182</td>\n",
              "      <td>185</td>\n",
              "      <td>186</td>\n",
              "      <td>187</td>\n",
              "      <td>190</td>\n",
              "      <td>...</td>\n",
              "      <td>90</td>\n",
              "      <td>77</td>\n",
              "      <td>88</td>\n",
              "      <td>117</td>\n",
              "      <td>123</td>\n",
              "      <td>127</td>\n",
              "      <td>129</td>\n",
              "      <td>134</td>\n",
              "      <td>145</td>\n",
              "      <td>152</td>\n",
              "      <td>156</td>\n",
              "      <td>179</td>\n",
              "      <td>105</td>\n",
              "      <td>106</td>\n",
              "      <td>105</td>\n",
              "      <td>104</td>\n",
              "      <td>104</td>\n",
              "      <td>104</td>\n",
              "      <td>175</td>\n",
              "      <td>199</td>\n",
              "      <td>178</td>\n",
              "      <td>152</td>\n",
              "      <td>136</td>\n",
              "      <td>130</td>\n",
              "      <td>136</td>\n",
              "      <td>150</td>\n",
              "      <td>118</td>\n",
              "      <td>92</td>\n",
              "      <td>85</td>\n",
              "      <td>76</td>\n",
              "      <td>92</td>\n",
              "      <td>105</td>\n",
              "      <td>105</td>\n",
              "      <td>108</td>\n",
              "      <td>133</td>\n",
              "      <td>163</td>\n",
              "      <td>157</td>\n",
              "      <td>163</td>\n",
              "      <td>164</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  ...  pixel781  pixel782  pixel783  pixel784\n",
              "0      3     107     118     127  ...       206       204       203       202\n",
              "1      6     155     157     156  ...       175       103       135       149\n",
              "2      2     187     188     188  ...       198       195       194       195\n",
              "3      2     211     211     212  ...       225       222       229       163\n",
              "4     13     164     167     170  ...       157       163       164       179\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JokA5cNhmap3",
        "colab_type": "code",
        "outputId": "d64455b7-650f-4dc5-f57e-b6cfcc575746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Train set shape:\", train.shape)\n",
        "print(\"Test set shape: \", test.shape)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set shape: (27455, 785)\n",
            "Test set shape:  (7172, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBWEIhn0nUKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train.drop(['label'], axis=1).values\n",
        "y_train = train['label'].values\n",
        "X_test = test.drop(['label'], axis=1).values\n",
        "y_test = test['label'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFQbVFELIj-d",
        "colab_type": "code",
        "outputId": "f93cfe1b-023e-4cd0-ed19-b8235ab6135a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27455, 784)\n",
            "(27455,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsvwgYeYnwxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B2iFgtQIqfM",
        "colab_type": "code",
        "outputId": "a20ede83-4c1d-4ca8-f0c7-de9e6ccd4732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27455, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVY4S6HBI5MW",
        "colab_type": "text"
      },
      "source": [
        "Some basic preprocessing of data to make training the models easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I40m_6Idn9PJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkUH18hkoFFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v3y-G3YI-ae",
        "colab_type": "text"
      },
      "source": [
        "So the labels range of 0 to 24, implying 25 possible letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ0SmoE-FNcZ",
        "colab_type": "code",
        "outputId": "fe5b1d99-be12-4e43-d351-dc8b7a56afbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_ind = 0\n",
        "for i in range(0, len(y_test)):\n",
        "  if y_test[i] > max_ind:\n",
        "    max_ind = y_test[i]\n",
        "print(max_ind)\n",
        "# SO 24 IS THE MAX INDICE"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2CRhugmJFxd",
        "colab_type": "text"
      },
      "source": [
        "Here we had to add dummies to create a numerical representation of categories. This was necessary for some of the models but not for others. Thus, we did two separate train_test_splits. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veda01hboH8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this whole get dummies thing is messing up a lot of models\n",
        "y_train_dummies = pd.get_dummies(y_train)\n",
        "y_test_dummies = pd.get_dummies(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWbKDrU1oQ2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, random_state=42, stratify=y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qoX1lsVJan3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val_dummies, y_train, y_val_dummies = train_test_split(X_train, y_train_dummies, random_state=42, stratify=y_train_dummies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYdWDWSjFMtX",
        "colab_type": "text"
      },
      "source": [
        "So we have multiple data subsets that we need to organize.\n",
        "\n",
        "\n",
        "1.   X_train and y_train match with dummies\n",
        "        - X_val_dummies and y_val_dummies\n",
        "2.   X_test and y_test dummies should be used for this set\n",
        "3.   X_train_sub and y_train_sub match with normal label sets\n",
        "        - X_val and y_val\n",
        "4.   X_test and y_test are pure - with one label\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oju0-g5o4xm",
        "colab_type": "code",
        "outputId": "aaed55e3-d64b-4c90-b5be-ec29d117076c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train[1].shape)\n",
        "# So images are 28x28x1, grayscale"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qbg3w9pAZOV",
        "colab_type": "text"
      },
      "source": [
        "# **Model 1 - CNN.** Below is the CNN model that inspired our work. We copied it from the website below and used it as a benchmark for our own models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUy-BEV0pzx5",
        "colab_type": "code",
        "outputId": "3cb7c6c9-1384-4204-84b3-6481611fb0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# CNN model from https://medium.com/the-research-nest/applied-machine-learning-part-2-a4ba715649d1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(8, (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "model.add(Conv2D(16, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(24, activation='softmax'))\n",
        "# model.add(Dense(1, activation='softmax'))\n",
        "# model.add(Dense(25, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 13, 13, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 11, 11, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 128)               51328     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 24)                3096      \n",
            "=================================================================\n",
            "Total params: 55,672\n",
            "Trainable params: 55,672\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaSKUsFQsNI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hseO5Uu1LKS9",
        "colab_type": "code",
        "outputId": "5ea6ec4a-b4c4-4cc3-f71b-c338ad6bd7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val_dummies.shape)\n",
        "print(y_val_dummies.shape)"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20591, 28, 28, 1)\n",
            "(20591, 24)\n",
            "(6864, 28, 28, 1)\n",
            "(6864, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4f4_PGqsY07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33e6eab1-722e-42b5-f1ec-fb60a4b60623"
      },
      "source": [
        "history = model.fit(X_train, y_train, validation_data = (X_val_dummies, y_val_dummies), epochs=50, batch_size=512)"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20591 samples, validate on 6864 samples\n",
            "Epoch 1/50\n",
            "20591/20591 [==============================] - 7s 351us/step - loss: 3.1090 - acc: 0.1054 - val_loss: 2.9388 - val_acc: 0.2823\n",
            "Epoch 2/50\n",
            "20591/20591 [==============================] - 6s 298us/step - loss: 2.5502 - acc: 0.2829 - val_loss: 1.9626 - val_acc: 0.4846\n",
            "Epoch 3/50\n",
            "20591/20591 [==============================] - 6s 298us/step - loss: 1.7749 - acc: 0.4538 - val_loss: 1.2982 - val_acc: 0.6761\n",
            "Epoch 4/50\n",
            "20591/20591 [==============================] - 6s 300us/step - loss: 1.3232 - acc: 0.5810 - val_loss: 0.9456 - val_acc: 0.7453\n",
            "Epoch 5/50\n",
            "20591/20591 [==============================] - 6s 299us/step - loss: 1.0409 - acc: 0.6740 - val_loss: 0.7138 - val_acc: 0.8207\n",
            "Epoch 6/50\n",
            "20591/20591 [==============================] - 6s 302us/step - loss: 0.8444 - acc: 0.7337 - val_loss: 0.5832 - val_acc: 0.8568\n",
            "Epoch 7/50\n",
            "20591/20591 [==============================] - 6s 300us/step - loss: 0.7117 - acc: 0.7753 - val_loss: 0.4614 - val_acc: 0.8814\n",
            "Epoch 8/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.5996 - acc: 0.8116 - val_loss: 0.3808 - val_acc: 0.9018\n",
            "Epoch 9/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.5250 - acc: 0.8322 - val_loss: 0.3157 - val_acc: 0.9245\n",
            "Epoch 10/50\n",
            "20591/20591 [==============================] - 6s 302us/step - loss: 0.4513 - acc: 0.8578 - val_loss: 0.2594 - val_acc: 0.9441\n",
            "Epoch 11/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.3947 - acc: 0.8759 - val_loss: 0.2150 - val_acc: 0.9540\n",
            "Epoch 12/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.3491 - acc: 0.8903 - val_loss: 0.1850 - val_acc: 0.9631\n",
            "Epoch 13/50\n",
            "20591/20591 [==============================] - 6s 302us/step - loss: 0.2966 - acc: 0.9080 - val_loss: 0.1444 - val_acc: 0.9748\n",
            "Epoch 14/50\n",
            "20591/20591 [==============================] - 6s 300us/step - loss: 0.2607 - acc: 0.9225 - val_loss: 0.1236 - val_acc: 0.9797\n",
            "Epoch 15/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.2334 - acc: 0.9298 - val_loss: 0.0976 - val_acc: 0.9848\n",
            "Epoch 16/50\n",
            "20591/20591 [==============================] - 6s 304us/step - loss: 0.2063 - acc: 0.9395 - val_loss: 0.0871 - val_acc: 0.9899\n",
            "Epoch 17/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.1786 - acc: 0.9510 - val_loss: 0.0637 - val_acc: 0.9924\n",
            "Epoch 18/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.1673 - acc: 0.9516 - val_loss: 0.0557 - val_acc: 0.9937\n",
            "Epoch 19/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.1447 - acc: 0.9594 - val_loss: 0.0501 - val_acc: 0.9955\n",
            "Epoch 20/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.1300 - acc: 0.9651 - val_loss: 0.0368 - val_acc: 0.9996\n",
            "Epoch 21/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.1177 - acc: 0.9680 - val_loss: 0.0323 - val_acc: 0.9980\n",
            "Epoch 22/50\n",
            "20591/20591 [==============================] - 6s 304us/step - loss: 0.1111 - acc: 0.9687 - val_loss: 0.0291 - val_acc: 0.9990\n",
            "Epoch 23/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.1007 - acc: 0.9720 - val_loss: 0.0229 - val_acc: 0.9993\n",
            "Epoch 24/50\n",
            "20591/20591 [==============================] - 6s 306us/step - loss: 0.0937 - acc: 0.9744 - val_loss: 0.0212 - val_acc: 0.9993\n",
            "Epoch 25/50\n",
            "20591/20591 [==============================] - 6s 300us/step - loss: 0.0868 - acc: 0.9768 - val_loss: 0.0171 - val_acc: 0.9997\n",
            "Epoch 26/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.0768 - acc: 0.9798 - val_loss: 0.0183 - val_acc: 0.9999\n",
            "Epoch 27/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.0776 - acc: 0.9800 - val_loss: 0.0125 - val_acc: 0.9994\n",
            "Epoch 28/50\n",
            "20591/20591 [==============================] - 6s 302us/step - loss: 0.0652 - acc: 0.9837 - val_loss: 0.0120 - val_acc: 0.9997\n",
            "Epoch 29/50\n",
            "20591/20591 [==============================] - 6s 304us/step - loss: 0.0660 - acc: 0.9824 - val_loss: 0.0109 - val_acc: 0.9997\n",
            "Epoch 30/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.0613 - acc: 0.9840 - val_loss: 0.0087 - val_acc: 0.9996\n",
            "Epoch 31/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.0545 - acc: 0.9862 - val_loss: 0.0102 - val_acc: 0.9991\n",
            "Epoch 32/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.0513 - acc: 0.9878 - val_loss: 0.0067 - val_acc: 0.9999\n",
            "Epoch 33/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.0500 - acc: 0.9866 - val_loss: 0.0074 - val_acc: 0.9997\n",
            "Epoch 34/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.0478 - acc: 0.9884 - val_loss: 0.0056 - val_acc: 0.9997\n",
            "Epoch 35/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.0453 - acc: 0.9888 - val_loss: 0.0052 - val_acc: 0.9999\n",
            "Epoch 36/50\n",
            "20591/20591 [==============================] - 6s 304us/step - loss: 0.0436 - acc: 0.9876 - val_loss: 0.0042 - val_acc: 1.0000\n",
            "Epoch 37/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.0409 - acc: 0.9887 - val_loss: 0.0041 - val_acc: 1.0000\n",
            "Epoch 38/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.0403 - acc: 0.9896 - val_loss: 0.0041 - val_acc: 0.9999\n",
            "Epoch 39/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.0378 - acc: 0.9911 - val_loss: 0.0034 - val_acc: 0.9999\n",
            "Epoch 40/50\n",
            "20591/20591 [==============================] - 6s 304us/step - loss: 0.0358 - acc: 0.9896 - val_loss: 0.0040 - val_acc: 0.9999\n",
            "Epoch 41/50\n",
            "20591/20591 [==============================] - 6s 303us/step - loss: 0.0371 - acc: 0.9900 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 42/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.0310 - acc: 0.9924 - val_loss: 0.0024 - val_acc: 1.0000\n",
            "Epoch 43/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.0328 - acc: 0.9919 - val_loss: 0.0032 - val_acc: 1.0000\n",
            "Epoch 44/50\n",
            "20591/20591 [==============================] - 6s 300us/step - loss: 0.0323 - acc: 0.9915 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 45/50\n",
            "20591/20591 [==============================] - 6s 304us/step - loss: 0.0272 - acc: 0.9935 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 46/50\n",
            "20591/20591 [==============================] - 6s 302us/step - loss: 0.0267 - acc: 0.9937 - val_loss: 0.0020 - val_acc: 1.0000\n",
            "Epoch 47/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.0274 - acc: 0.9922 - val_loss: 0.0020 - val_acc: 0.9999\n",
            "Epoch 48/50\n",
            "20591/20591 [==============================] - 6s 300us/step - loss: 0.0258 - acc: 0.9934 - val_loss: 0.0018 - val_acc: 1.0000\n",
            "Epoch 49/50\n",
            "20591/20591 [==============================] - 6s 301us/step - loss: 0.0252 - acc: 0.9935 - val_loss: 0.0016 - val_acc: 1.0000\n",
            "Epoch 50/50\n",
            "20591/20591 [==============================] - 6s 305us/step - loss: 0.0252 - acc: 0.9934 - val_loss: 0.0017 - val_acc: 0.9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjG_tMuisdLj",
        "colab_type": "code",
        "outputId": "29331f50-d3a8-4087-975b-88106e5fa487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train','validation (not full test set yet)'])\n",
        "plt.show()"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngTHGuNz/B6mvvGKLsFOzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crO7-Yr7uso9",
        "colab_type": "code",
        "outputId": "8f04a9e8-6c8c-46d5-9e62-9698764a3840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# X_test_yo = np.reshape(X_test, (7172, 28*28))\n",
        "# predictions = model.predict(X_test_yo)\n",
        "print(X_test.shape)\n",
        "print(y_test_dummies.shape)"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7172, 28, 28, 1)\n",
            "(7172, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWTvEaO8vqjh",
        "colab_type": "code",
        "outputId": "f0dc24c4-745e-4afd-cb7a-0262c7a54193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "score = model.evaluate(X_test, y_test_dummies, verbose=1)\n",
        "print('test loss: ', score[0])\n",
        "print('test accuracy: ', score[1])"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7172/7172 [==============================] - 1s 183us/step\n",
            "test loss:  0.29532248571377956\n",
            "test accuracy:  0.9287506971556051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5iytnOvW6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# test_accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))\n",
        "# print(\"The test accuracy is: \", test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzwkjFZ25qQi",
        "colab_type": "code",
        "outputId": "e019da9b-1daa-465c-c697-2a5fc92903f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20591, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab7xrCcHvaHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.svm import SVC\n",
        "# svm = SVC(kernel=\"rbf\")\n",
        "X = X_train.copy()\n",
        "y = y_train.copy()\n",
        "X = np.reshape(X, (20591, 28*28))\n",
        "\n",
        "# y_store = np.zeros((y.shape[0],1))\n",
        "# for i in range(0, y.shape[0]):\n",
        "#   print(\"i\", i, \"y[i]\", y[i])\n",
        "#   # y_store[i] = np.argmax(y[i])\n",
        "# # X = X.reshape(-1, 1)\n",
        "# X = np.reshape(X, (15443, 28*28))\n",
        "# # y = y.flatten(0)\n",
        "# print(X.shape)\n",
        "# print(y.shape)\n",
        "# # y = y.reshape(1,-1)\n",
        "# svm.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjT4BMDHAPYX",
        "colab_type": "text"
      },
      "source": [
        "# **Model 2 - MLP Classifier.** Below is our MLP Classifier that achieved a test accuracy of 58.9%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqj7cxRbx_YY",
        "colab_type": "code",
        "outputId": "2bb8e648-0f26-459e-b5c0-d5b4c4ed182e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "neural_net = MLPClassifier()\n",
        "# neural_net.fit(X_train.reshape(1, -1), y_train.reshape(1,-1))\n",
        "neural_net.fit(X, y)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
              "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
              "              validation_fraction=0.1, verbose=False, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wFAFcmt207B",
        "colab_type": "code",
        "outputId": "f10a13d4-7a71-4686-b940-cc3e5b2522a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_test.shape)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7172, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxMeP9E2yLhD",
        "colab_type": "code",
        "outputId": "f4d34f6d-3774-4810-ebb9-b861c0bbd97d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "X_test_reshape = np.reshape(X_test, (7172, 28*28))\n",
        "y_pred = neural_net.predict(X_test_reshape)\n",
        "X_val_2 = X_val_dummies.copy()\n",
        "X_val_2 = np.reshape(X_val_2, (6864, 28*28))\n",
        "y_pred_2 = neural_net.predict(X_val_2)\n",
        "y_val_2 = y_val_dummies.copy()\n",
        "print(\"MLP Classifier's Test Set Accuracy:  \", 100*accuracy_score(y_val_2, y_pred_2), \"%\")\n",
        "print(\"MLP Classifier's Test Set Accuracy:  \", 100*accuracy_score(y_test_dummies, y_pred), \"%\")"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP Classifier's Test Set Accuracy:   99.83974358974359 %\n",
            "MLP Classifier's Test Set Accuracy:   58.88176240936978 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zlGImS62p89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# k_neighbors = KNeighborsClassifier(n_neighbors = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8XQfzj8AIf-",
        "colab_type": "text"
      },
      "source": [
        "# **Model 3 - Simple Neural Network.** Below is our simple neural network from scratch. Its test accuracy is incredibly low ~2-3%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdYUx8Bh6C4M",
        "colab_type": "code",
        "outputId": "4da1f908-e91f-4677-bae5-04e780d1373b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "X_train_2 = np.reshape(X_train_sub, (X_train_sub.shape[0], 28*28))\n",
        "y_train_2 = y.copy()\n",
        "# y_train_2 = y_train_sub.copy()\n",
        "print(\"y_train_2 shape\", y_train_2.shape)\n",
        "inp = keras.layers.Input(shape=X_train_2.shape[1:])\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(inp)\n",
        "# hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "# concat = keras.layers.concatenate([input_, hidden1])\n",
        "output = keras.layers.Dense(24)(hidden1)\n",
        "model = keras.models.Model(inputs=[inp], outputs=[output])"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train_2 shape (20591, 24)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkQUgWwV9Ikz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_val_2 = np.reshape(X_val_2, (6864, 28*28))\n",
        "# print(X_val_2.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dxGCZ0y-BK4",
        "colab_type": "code",
        "outputId": "b73c0b92-3d9b-4c43-c371-754a32823add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(y_val_2.shape)\n",
        "learning_rate = 0.3\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=learning_rate), metrics = [\"accuracy\"])\n",
        "history = model.fit(X_train_2, y_train_2, epochs=40, validation_data=(X_val_2, y_val_2))"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6864, 24)\n",
            "Train on 20591 samples, validate on 6864 samples\n",
            "Epoch 1/40\n",
            "20591/20591 [==============================] - 2s 92us/step - loss: 0.0444 - acc: 0.0421 - val_loss: 0.0399 - val_acc: 0.0408\n",
            "Epoch 2/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0442 - val_loss: 0.0399 - val_acc: 0.0459\n",
            "Epoch 3/40\n",
            "20591/20591 [==============================] - 1s 55us/step - loss: 0.0399 - acc: 0.0451 - val_loss: 0.0399 - val_acc: 0.0473\n",
            "Epoch 4/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0444 - val_loss: 0.0399 - val_acc: 0.0473\n",
            "Epoch 5/40\n",
            "20591/20591 [==============================] - 1s 55us/step - loss: 0.0399 - acc: 0.0460 - val_loss: 0.0399 - val_acc: 0.0420\n",
            "Epoch 6/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0446 - val_loss: 0.0399 - val_acc: 0.0437\n",
            "Epoch 7/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0464 - val_loss: 0.0399 - val_acc: 0.0452\n",
            "Epoch 8/40\n",
            "20591/20591 [==============================] - 1s 55us/step - loss: 0.0399 - acc: 0.0434 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 9/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0457 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 10/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0436 - val_loss: 0.0399 - val_acc: 0.0460\n",
            "Epoch 11/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0434 - val_loss: 0.0399 - val_acc: 0.0446\n",
            "Epoch 12/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0469 - val_loss: 0.0399 - val_acc: 0.0436\n",
            "Epoch 13/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0446 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 14/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0432 - val_loss: 0.0399 - val_acc: 0.0460\n",
            "Epoch 15/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0453 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 16/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0457 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 17/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0434 - val_loss: 0.0399 - val_acc: 0.0460\n",
            "Epoch 18/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0443 - val_loss: 0.0399 - val_acc: 0.0460\n",
            "Epoch 19/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0440 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 20/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0439 - val_loss: 0.0399 - val_acc: 0.0422\n",
            "Epoch 21/40\n",
            "20591/20591 [==============================] - 1s 58us/step - loss: 0.0399 - acc: 0.0440 - val_loss: 0.0399 - val_acc: 0.0424\n",
            "Epoch 22/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0467 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 23/40\n",
            "20591/20591 [==============================] - 1s 58us/step - loss: 0.0399 - acc: 0.0423 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 24/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0425 - val_loss: 0.0400 - val_acc: 0.0452\n",
            "Epoch 25/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0437 - val_loss: 0.0399 - val_acc: 0.0436\n",
            "Epoch 26/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0432 - val_loss: 0.0399 - val_acc: 0.0417\n",
            "Epoch 27/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0435 - val_loss: 0.0399 - val_acc: 0.0462\n",
            "Epoch 28/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0423 - val_loss: 0.0399 - val_acc: 0.0460\n",
            "Epoch 29/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0440 - val_loss: 0.0399 - val_acc: 0.0439\n",
            "Epoch 30/40\n",
            "20591/20591 [==============================] - 1s 55us/step - loss: 0.0399 - acc: 0.0443 - val_loss: 0.0399 - val_acc: 0.0460\n",
            "Epoch 31/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0454 - val_loss: 0.0399 - val_acc: 0.0437\n",
            "Epoch 32/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0439 - val_loss: 0.0399 - val_acc: 0.0439\n",
            "Epoch 33/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0448 - val_loss: 0.0399 - val_acc: 0.0452\n",
            "Epoch 34/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0430 - val_loss: 0.0399 - val_acc: 0.0472\n",
            "Epoch 35/40\n",
            "20591/20591 [==============================] - 1s 58us/step - loss: 0.0399 - acc: 0.0440 - val_loss: 0.0399 - val_acc: 0.0447\n",
            "Epoch 36/40\n",
            "20591/20591 [==============================] - 1s 58us/step - loss: 0.0399 - acc: 0.0463 - val_loss: 0.0399 - val_acc: 0.0462\n",
            "Epoch 37/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0454 - val_loss: 0.0399 - val_acc: 0.0462\n",
            "Epoch 38/40\n",
            "20591/20591 [==============================] - 1s 57us/step - loss: 0.0399 - acc: 0.0440 - val_loss: 0.0399 - val_acc: 0.0437\n",
            "Epoch 39/40\n",
            "20591/20591 [==============================] - 1s 56us/step - loss: 0.0399 - acc: 0.0431 - val_loss: 0.0399 - val_acc: 0.0424\n",
            "Epoch 40/40\n",
            "20591/20591 [==============================] - 1s 58us/step - loss: 0.0399 - acc: 0.0437 - val_loss: 0.0399 - val_acc: 0.0460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTnk-_iV6Han",
        "colab_type": "code",
        "outputId": "0153ba74-e79f-42ea-cc93-fe889c4f268c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y_test.shape)\n",
        "X_test_2 = np.reshape(X_test, (7172, 28*28))\n",
        "y_test_2 = np.reshape(y_test_dummies, (y_test_dummies.shape[0], 24))\n",
        "# history = model.fit(X_train_2, y_train_2, epochs=40, validation_data=(X_test_2, y_test_2))"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7172,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVEu2-2kZf4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a6734b65-b695-445e-e486-51ede38f25a7"
      },
      "source": [
        "score = model.evaluate(X_test_2, y_test_dummies, verbose=1)\n",
        "print(\"Our simple Neural Network's test loss: \", score[0])\n",
        "print(\"Our simple Neural Network's test accuracy:\", score[1])"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7172/7172 [==============================] - 0s 27us/step\n",
            "Our simple Neural Network's test loss:  0.04001228104522623\n",
            "Our simple Neural Network's test accuracy: 0.022587841606246516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJki-MxT_86O",
        "colab_type": "text"
      },
      "source": [
        "# **Model 4 - Random Forest Classifier.** Below is our Random Forest Classifier whose performance is also atrocious. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Xzrfpw8o4u",
        "colab_type": "code",
        "outputId": "ebc444ea-03ad-4519-cde7-d064a7475f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# MY ATTEMPT AT A RANDOM FOREST CLASSIFIER\n",
        "# A random forest calssifier is a an ensemble of decision trees\n",
        "#   that are usually trained with bagging.\n",
        "#   Usually RFC train by splitting on the best feature\n",
        "#   but we can introduce extra randomness by searching\n",
        "#   for the best feature among a random subset of features\n",
        "#   this leads to greater tree diversity. This trades higher bias for lower variance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_classifier = RandomForestClassifier(n_estimators=500,\n",
        "                                           max_leaf_nodes=24,\n",
        "                                           n_jobs=-1,\n",
        "                                           random_state=42)\n",
        "# history = random_classifier.fit(X_train_2,y_train_2, validation_data=(X_test_2, y_test_2))\n",
        "random_classifier.fit(X_train_2,y_train_2)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features='auto', max_leaf_nodes=30,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                       n_jobs=-1, oob_score=False, random_state=42, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiM5v9kJBIzD",
        "colab_type": "code",
        "outputId": "dca15e54-6830-4a57-d189-8dace409d2e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rando_predict = random_classifier.predict(X_test_2)\n",
        "rando_predict_train = random_classifier.predict(X_train_2)\n",
        "print(\"Our Random Forest Classifier's Train Accuracy:  \", accuracy_score(y_train_2, rando_predict_train))\n",
        "print(\"Our Random Forest Classifier's Test Accuracy:  \", accuracy_score(y_test_2, rando_predict))"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our Random Forest Classifier's Train Accuracy:   0.0\n",
            "Our Random Forest Classifier's Test Accuracy:   0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GenOF2yN_38t",
        "colab_type": "text"
      },
      "source": [
        "# **Model 5 - Bagging Classifier.** Below is our Bagging Classifier that has a test accuracy of 69.9%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK5HpxiU_8A6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "038c8518-5d20-4e52-961e-57e7cf5ec36b"
      },
      "source": [
        "# MY ATTEMPT AT A BAGGING CLASSIFIER\n",
        "# BAGGING TAKES RANDOM SAMPLES FROM THE TRAINING SET\n",
        "# AND TRAINS ON THOSE, AND PUTS THOSE SAMPLES BACK\n",
        "# IT DOES THIS MULTIPLE TIMES SO THAT IT TRAINS MULTIPLE MODELS\n",
        "# THEN USES THE VOTES FROM ALL THESE MODELS TO DECIDE OUTPUT\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# print(X_train_2.shape)\n",
        "X_test_2 = np.reshape(X_test, (7172, 28*28))\n",
        "X_train_sub_2 = np.reshape(X_train_sub, (20591, 28**2))\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "                           max_samples=1000, bootstrap=True, n_jobs=-1,\n",
        "                           random_state=42)\n",
        "bagging_classifier.fit(X_train_sub_2, y_train_sub)\n",
        "y_pred_bag = bagging_classifier.predict(X_test_2)\n",
        "print(\"Our Bagging Classifier's Test Accuracy:  \", accuracy_score(y_test, y_pred_bag))"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our Bagging Classifier's Test Accuracy:   0.6985499163413273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjiOXHyqfBY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32ebf1c9-f722-478a-e86d-30bb882fefe0"
      },
      "source": [
        "print(X_test.shape)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7172, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q7BcReJ_yAi",
        "colab_type": "text"
      },
      "source": [
        "# **Model 6 - Voting Classifier.** Below is our Voting Classifier that achieved test accuracy of 74.1%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv30Pttg_H1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "d4448545-0e30-4fc5-e4d2-d13e0f213680"
      },
      "source": [
        "# OUR ATTEMPT AT A SOFT VOTING CLASSIFIER\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "log_clf = LogisticRegression(random_state=42)\n",
        "rnd_clf = RandomForestClassifier(random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), \n",
        "                ('rf', rnd_clf),\n",
        "                ('svc', svm_clf)],\n",
        "                voting='soft')\n",
        "voting_clf.fit(X_train_sub_2, y_train_sub)\n",
        "vote_pred = voting_clf.predict(X_test_2)\n",
        "print(\"Our Voting Classifier's Test Accuracy:\", accuracy_score(y_test, vote_pred))"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Our Voting Classifier's Test Accuracy: 0.7409369771332962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP_JPmQn_kzb",
        "colab_type": "text"
      },
      "source": [
        "# **Predicting Images Examples.** Below we present two examples of the Voting Classifier correctly predicting labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS9JmMcm8yPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "4760a2e2-e6b7-46c9-9efa-37ca852e94c0"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "categories = ['A','B','C','D','E','F',\n",
        "              'G','H','I','J','K','L','M','N',\n",
        "              'O','P','Q','R','S','T','U','V',\n",
        "              'W','X','Y','Z']\n",
        "indy = 2\n",
        "image = X_test[indy]\n",
        "image = image.squeeze()\n",
        "plt.figure()\n",
        "plt.title('This is a %s.' % categories[y_test[indy]])\n",
        "plt.imshow(image); plt.grid(False); plt.axis('off'); plt.show()\n",
        "print('And the model predicts it as a', categories[vote_pred[indy]])"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3DmBpAgnkBThBJIinEBShBNIinACSf0vsJwfpGmkaVcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the model predicts it as a K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs_X_5kb_Q5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "0ad896aa-7f90-493e-f0d0-34a2dd6a88a7"
      },
      "source": [
        "indy = 4\n",
        "image = X_test[indy]\n",
        "image = image.squeeze()\n",
        "plt.figure()\n",
        "plt.title('This is a %s.' % categories[y_test[indy]])\n",
        "plt.imshow(image); plt.grid(False); plt.axis('off'); plt.show()\n",
        "print('And the model predicts it as a', categories[vote_pred[indy]])"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv6d4Z3QY9GTYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the model predicts it as a D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LicthuQ8_q7L",
        "colab_type": "text"
      },
      "source": [
        "And below we have two examples where the Voting Classifier incorrectly predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtjGAC0M_VMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "998c406a-21cb-488a-ba81-0a3d120e8278"
      },
      "source": [
        "indy = 5\n",
        "image = X_test[indy]\n",
        "image = image.squeeze()\n",
        "plt.figure()\n",
        "plt.title('This is a %s.' % categories[y_test[indy]])\n",
        "plt.imshow(image); plt.grid(False); plt.axis('off'); plt.show()\n",
        "print('And the model predicts it as a', categories[vote_pred[indy]])"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKcIJJEU4gaQIJ5AU4QSS+j80UtOeDHv+GgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the model predicts it as a W\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXgwMknx_Wd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "44678d8f-4bc1-4e52-db0b-fc91d8422aca"
      },
      "source": [
        "indy = 12\n",
        "image = X_test[indy]\n",
        "image = image.squeeze()\n",
        "plt.figure()\n",
        "plt.title('This is a %s.' % categories[y_test[indy]])\n",
        "plt.imshow(image); plt.grid(False); plt.axis('off'); plt.show()\n",
        "print('And the model predicts it as a', categories[vote_pred[indy]])"
      ],
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAARcElEQVR4nO3da4yd11UG4Hed65y5eK72uE48di6O\n41RJUNKkCqRt2lSkaqEUIWhBaZAKEm2FBD8QqBKCgABRqaoUqUj8KEgJEAI1KlAKJKKXkJQWajdN\njB07Ttypx449voztmfHczmXzwxNkRd7vmpyPcRbt+0iREq/Z3/k857z+nFlae1tKCSIST+nNvgER\nuTKFUyQohVMkKIVTJCiFUyQohVMkKIXzKjKzh83sL0l9v5nd9wav+Q4zO1T45iScypt9Az9IzGz+\nsv/sBbAMoL3637/irU8pvfWNvmZK6RkAO9/ourUys2sAfB/AzpTSK6+rfRHAKyml31iv1/9hpifn\n/6GUUv9r/wA4CuAnL/u1v3qz768bKaXjAL4C4KOX/7qZjQB4P4BH34z7+mGgcF59NTN7zMzmVv8a\n+7bXCmY2aWbvXf33u81sj5nNmtm0mX32Shczs/vM7Nhl//1bZnZ89fqHzOz+zLoPmNlzq9efMrOH\nyT0/iteFE8BHABxIKe1b4+9b3iCF8+r7IIAnAAwB+EcAn8t83SMAHkkpbQBwA4C/9S5sZjsB/CqA\nu1JKAwAeADCZ+fKLAB5avY8PAPiEmX0o87VfBDBmZvde9msfhZ6a60rhvPqeTSn9c0qpDeAvANye\n+bomgBvNbCylNJ9S+tYart0GUAdwi5lVU0qTr///xNeklL6eUtqXUuqklF4A8NcA3pX52kUAX8Cl\nMMPMdgC4E8Dja7gn6ZLCefWdvOzfFwD0mNmVfjD3SwBuAnDQzL5tZj/hXTil9DKAXwfwMIBTZvaE\nmW250tea2dvN7GtmdtrMLgD4OIAxcvlHAfysmfXg0lPzyZTSKe+epHsKZ1AppcMppZ8HsAnApwHs\nNrO+Nax7PKV0L4BtANLq2it5HJf+Wr01pTQI4E8BGLn0swBmAPwUgAehv9KuO4UzKDN70Mw2ppQ6\nAM6v/nLHWbPTzN5jZnUASwAWyZoBADMppSUzuxvAL7Brp0uzhY/hUtiHAHxp7b8b6YbCGdf7AOxf\n7Z0+AuAjq//vx9QB/DGAM7j01+dNAD6V+dpPAvh9M5sD8DtYww+ccCmcEwD+JqW0fHlh9SfDH17D\nNWSNTMPWIjHpySkSlMIpEpTCKRKUwikSFJ1K2f65z9CfFqWq88OkEqlX+Fore3XaVYCRP3ZKJb62\nUm3TulmxH6KllG8ndjqs1Qg06k1ab3WK/Xlb5PfWbvPXLrHPQ0EV5z1tk+85ALRa5a5f23vPOs73\n5eUP//YVL6Anp0hQCqdIUAqnSFAKp0hQCqdIUAqnSFAKp0hQfPc9L7q8vUPXr2cfEwBK5XyvslJx\nru30+rx+ndfvG+zLD5eM985nawDw/P5ttD6wZY7W6xXew207PTumXLD/u57ce/O+L+Q9rVb556nj\nfJZz9OQUCUrhFAlK4RQJSuEUCUrhFAlK4RQJSuEUCYr2OZPTi0SButvHdK7tzmSSXmbRPmaV9FDX\n4obBs9naQ5u+Qdd+8r9+mdYXDg/R+vgdx2h9brlO61G1nTlWr39b93qRpA/acvrarS6fgXpyigSl\ncIoEpXCKBKVwigSlcIoEpXCKBOWMjBVrpbB2iTvyVaBVAgBl9tpOK8XbZrHmjBfVqy1aP3BmPH/t\ncX7tzbdO0/rS3+WvDQDndzVovb++nK0ttfjHxRvL8ranLHJtT8PZ7vTsfC+tLy9Xs7WxIT7m117J\nr2X05BQJSuEUCUrhFAlK4RQJSuEUCUrhFAlK4RQJijeuHOb0QVnd62N6Y1ter5LVi/YxvdeuOtfP\ndxKBL1+4na59aNu3aP3Tuz5I652jfKRs4678SJk3llX23tMCxxN61/Z6qO/e/BKtP7bnHlof2lvL\n1ho/fY6u9b5vOXpyigSlcIoEpXCKBKVwigSlcIoEpXCKBKVwigRV6AjAIttblpy1Xp/Tq7NeptfH\n9PqgXk+t49TZvOf+C2+haz8++gytP3XXy7S+/ys30fq56/LznsM9+aMLAaDZKdN6rdT9lqKLLT4T\nOdpYoPWJWn47UgAoz/Drd/JtTjQqTbp2vsvtRvXkFAlK4RQJSuEUCUrhFAlK4RQJSuEUCUrhFAnK\n2YjUm9fkF2czm0XnNb1eZJlc31tb8o4IdOre7GEf6ffNLvfQtafbfN/Zd40cpvV9lR20fvLYSLa2\nddcRuvbCMr83D/u+eb3jO4eP0vp0c5DWG6f49ee3kc9ywc9D9rpdrRKRdadwigSlcIoEpXCKBKVw\nigSlcIoERVspJefYNHgtB9LO8Fol7Ag/AKg49W5/fL0W9Qo/4s/D7s0bR3theSut76y/SusrY/w9\nrU2Tj8QuurTw95y1JMYaF+nae/r4qNznT7yDvzaf+kLfdRf4FxCNqnPxDD05RYJSOEWCUjhFglI4\nRYJSOEWCUjhFglI4RYKifc4iI2GX6uwIQN7nrHvbV5a732bRG/Hx+pglFBsRYq/fX1uhaw8tbKb1\nTRtmaX3zNr5F5PyR8WxtoUX2hwTQW+H37plbyY/L7Ro+Sde+srKJ1p+b5P3hAefjdN3wTLbmbdvp\njSjm6MkpEpTCKRKUwikSlMIpEpTCKRKUwikSlMIpEhTfGrPAvCbAZzKrTp/S6xWWC2xfWbSP6d17\nxfi9s77YrkHez/u1jf9O67tnb6N17xi/zsn87/3gt7fTte+77zu0fmR+lNYH6/l7u6ufb8v57Cw/\n2rB3H9+28+K1/D0frefnSY+1huha7/OUXyciISmcIkEpnCJBKZwiQSmcIkEpnCJBKZwiQfF9awvu\nLcv2pmVH9HlrgWIzmUXnMb0+ZivxP/O29OX3QB2t8v1ZZ9p8drC/vETr8yt1Wj9H9qYd+y7/vv3n\nrm20ftsY31N3vJ6fRe0xvvfrkfkxWq+d5/c++J5pWl/u5KPSqPB7W+mUaT1HT06RoBROkaAUTpGg\nFE6RoBROkaAUTpGgFE6RoHif0+ljFuH1EqtOvchMZpF9ZQGgUuLznL0lvn/rZ695Mlv7g1P30rV/\ndPz9tP6ZiX+g9cMb8/vSAsDxd57P1o7czucx5/ZspPVv3Mx7rPdMTGZrT3dupmsPPM97rI1Rfu7p\nj41N0fqri4PZWq1c7LzWHD05RYJSOEWCUjhFglI4RYJSOEWCUjhFguKtlAJbXwL8GD9vJMxtdxQY\n+/JaJd5Rdp3Efyy/vZcfs/fkwjXZ2l3936Nrzzd7af1nXvgYrW/sm6f135z412zta/1kngzAF77J\nj+Eb3t1H61+/j1zfeYxs2svfk7MP8C1BPewz440QostWi56cIkEpnCJBKZwiQSmcIkEpnCJBKZwi\nQSmcIkHRPqfbi3TqRVS8IwALjH31lPlWhl7faqFdo/X+8jKtH1nO9wN3T/4If+3vjtD6tn/ifczF\n4Q20/vnffWe2dkPfabp2aaNzrOM+55i9vfktJNt13scstfh7dvf2SVqfa/XQes0ZE2Qq6G70Uk9O\nkaAUTpGgFE6RoBROkaAUTpGgFE6RoBROkaAKzXMWmcn0tr705jU9rFfp9azqzvydd6TbYGWB1o8s\n5reQXNzLt590vm04e2s/rQ9O8lnVPf+Wn6n8xINfpWuf2sG3r5w7xLflHD6cvzdnhBbndvLeszdj\ne2I5v/UlAFTJZ6bpfB68+eHsuq5Wici6UzhFglI4RYJSOEWCUjhFglI4RYJSOEWCon1OT7lAH9Sb\nx/R4vSN2LFu3faf/vbbTJx0t85lKNPKlOx84QJfuObaV1pvzA7R+6g5+DF8PGdmcavIe7IeufYHW\n/3zLe2l9+HC+1jPNe8cL767S+mCF71s7vcznXNmxj0V78vnrikhICqdIUAqnSFAKp0hQCqdIUAqn\nSFAKp0hQtM9ZdO9YNrPp9Qq9axfZe5bN5gHF+1a31l+l9R7Lv/7u43fQtSsn+BmXVd7uw8JbnDna\nlfzgZE+Jf88f6P9vWv+z63+U1i8eyp892jPFX7td5+9ZlXzPAd7HBIB6Kd83ZzUA6KTunoF6cooE\npXCKBKVwigSlcIoEpXCKBKVwigTFWyll/mP3IltjeryxrqJjX0zDadN4Bpwfy39pPr/95ORUfttM\nACiTVgcAXLzWaRlsXKL1kcGL2dqPN2bo2heb/N42j8zS+tlr8m2i4QN81K28zF+77rSBiui2VeLR\nk1MkKIVTJCiFUyQohVMkKIVTJCiFUyQohVMkKNrnbHd476jCTz6jvB4oG/laS531QYuOhHl90KZz\nebZ15vatZG9KAOdHyb6aANrOWXneGODPTXwnW+st8WP2plr8+MFmhz8LOuTyqeScAVhQ1fk8MSVn\nrUbGRH7AKJwiQSmcIkEpnCJBKZwiQSmcIkEpnCJB0T5ncnpmXp1pOz0vODORng65tw6K9cwutnm/\nr+pcnm2duX3Am5kcp/WB+jKtjzfmaP3tvS+TKn/Pljp8X87ZhR5ab/XmG8SdGm+qG9+d0t0a08PW\nNxO/N68Pml3X1SoRWXcKp0hQCqdIUAqnSFAKp0hQCqdIUAqnSFCF9q31ZgMZd56zYJ+zyDxnt32p\n18y0eb/vumr++jf3naRrD5/n+9oO1xdo/YY+Pi96SzW/r2078T7lUJm/dtnbB5m8LanqzII6Rx+W\n4c3/dv+eez1Urw+aoyenSFAKp0hQCqdIUAqnSFAKp0hQCqdIULSV4rU7qmX+I+RGJb+FZM1plRQ9\n4q9Kru+1abwfjXutmIuJ/1x/sJT/M7G/zI/omxg4R+stZxvGtzaO0fpwuTdbe6mZPx4QAI42r6X1\nVqv7sS9r8e95u6fY56VecmbOCqiiu7agnpwiQSmcIkEpnCJBKZwiQSmcIkEpnCJBKZwiQdE+pxXs\nNTJeP45vPun3QVkv0jvuzet5DVX5aNTp9gZaB/JHALLjAQHg/pEXaf2VpU20vqHE+6jMQod+XHB8\nZZjWvc9TZSG/p6i1+XtWXuL7kQ6UF2nd622zz0TRrVZz9OQUCUrhFAlK4RQJSuEUCUrhFAlK4RQJ\nSuEUCYo3rrzFBbbG7K2s8Gs7vUg2rwkAfZX8UXhFj4Pz+qBTK6O03u6dzdZ21KbpWm+bxdEK75Me\nb/Je5Ln2kWxt79L1dO3RxRFabzb5vTdIK7K04szY5keHAQA15z33Pk9FjgDslp6cIkEpnCJBKZwi\nQSmcIkEpnCJBKZwiQSmcIkHRPme1QB8TKL73LFNzeo2sL+Xdl9fz8pxYGaT1g818D3ZrhX/PT7f5\nvZ3v8IbfmdYArT9N5kH/48KNdO3Bc3yWtO30OUv5bwtsib/f3r61JecIwDrbNBe8lzlS4fv5LnS8\n6eQr05NTJCiFUyQohVMkKIVTJCiFUyQohVMkKIVTJCja55xbqtPFzZqT7e7aOwCADVW+v2qrw3tm\nzVK+7vW0PG5PDPzeplpD2dqNVT6PebrNX/v5xQlaf/rMDlqf6s/PZE4v8R7pubn82Z4AUK3ze68s\n5XuVpTneSxy+mfcxN5XnaH28eoHW2b63T87cStd+88u30fqnfu/Kv64np0hQCqdIUAqnSFAKp0hQ\nCqdIUAqnSFB8a8yn+FaHi060F8jV27xLA+eEQLfeqeV/LO+tLcrbKfGJLW/L1p67/0/o2sPNjbT+\nzFk+1vXi97bQ+uSG/Ht+/dhZurbVcn7jiR+VV7/Q/YjimWl+7OLHln6R1jsdfm+1Wr4N1PP3+dYY\nAGw9wNs4UCtF5P8XhVMkKIVTJCiFUyQohVMkKIVTJCiFUyQo2uccOUj2KgSQyrw3VESpyXtenSr/\nc6VTIffm7NhZXuGv3Wo4R9mdXODXPzGTrf3LV3kf8tnZm2j9pRN8e0qb463tlXo1W/OOZWxfcGYE\nne9774n8mGDr+1N07S1/yN+T5Qnes18ZdE7DJB+n/pd4/zeVunsG6skpEpTCKRKUwikSlMIpEpTC\nKRKUwikSlMIpEpSltH7H9IlI9/TkFAlK4RQJSuEUCUrhFAlK4RQJSuEUCep/ACfPcHfRkySjAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the model predicts it as a R\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}